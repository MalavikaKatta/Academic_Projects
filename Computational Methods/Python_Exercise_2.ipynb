{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalavikaKatta/Academic_Projects/blob/main/Computational%20Methods/Python_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "## Question 1 (5 points)\n",
        "\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmNR3kw9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ccb17f2e-3642-4788-e2d2-c71220dc4ca3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI want to know the weather report for the next year. I need at least the past 20 years of data related to the weather conditions of each day. The more data, accurate of prediction is high. Using some search engines, most of the data can be collected.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "I want to know the weather report for the next year. I need at least the past 20 years of data related to the weather conditions of each day. The more data, accurate of prediction is high. Using some search engines, most of the data can be collected.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "## Question 2 (5 points):\n",
        "\n",
        "Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae773c57-2f96-4bba-dfeb-f564f85fb063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1000 data samples.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import datetime\n",
        "start_date = datetime.date.today() - datetime.timedelta(days=20*365)\n",
        "end_date = datetime.date.today()\n",
        "data_samples = []\n",
        "num_samples_to_collect = 1000\n",
        "while len(data_samples) < num_samples_to_collect:\n",
        "    random_date = start_date + datetime.timedelta(days=random.randint(0, (end_date - start_date).days))\n",
        "    temperature = random.uniform(0, 40)\n",
        "    humidity = random.uniform(0, 100)\n",
        "    data_sample = {\n",
        "        \"date\": random_date.strftime(\"%Y-%m-%d\"),\n",
        "        \"temperature\": temperature,\n",
        "        \"humidity\": humidity\n",
        "    }\n",
        "    data_samples.append(data_sample)\n",
        "print(f\"Collected {len(data_samples)} data samples.\")\n",
        "for i in range(0,10):\n",
        "  print(data_samples[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "## Question 3 (5 points):\n",
        "\n",
        "Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622a4228-99cc-464a-f9c6-e15b6cfc963d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            "Title: Information retrieval as statistical translation\n",
            "Authors: A Berger, J Lafferty - ACM SIGIR Forum, 2017 - dl.acm.org\n",
            "Year: dl.acm.org\n",
            "Venue: A Berger, J Lafferty\n",
            "Abstract: … There is a large literature on probabilistic approaches to information retrieval, and we will \n",
            "not attempt to survey it here. Instead, we focus on the language modeling approach introduced …\n",
            "\n",
            "Article 2:\n",
            "Title: [BOOK][B] Information retrieval: Implementing and evaluating search engines\n",
            "Authors: S Buttcher, CLA Clarke, GV Cormack - 2016 - books.google.com\n",
            "Year: books.google.com\n",
            "Venue: S Buttcher, CLA Clarke, GV Cormack\n",
            "Abstract: … Information retrieval forms the foundation for modern search engines. In this textbook we \n",
            "provide an introduction to information retrieval targeted at graduate students and working …\n",
            "\n",
            "Article 3:\n",
            "Title: A language modeling approach to information retrieval\n",
            "Authors: JM Ponte, WB Croft - ACM SIGIR Forum, 2017 - dl.acm.org\n",
            "Year: dl.acm.org\n",
            "Venue: JM Ponte, WB Croft\n",
            "Abstract: … models, we have developed an approach to retrieval based on probabilistic language … in \n",
            "information retrieval in two senses. The first sense denotes an abstraction of the retrieval task …\n",
            "\n",
            "Article 4:\n",
            "Title: A study of smoothing methods for language models applied to ad hoc information retrieval\n",
            "Authors: C Zhai, J Lafferty - ACM SIGIR Forum, 2017 - dl.acm.org\n",
            "Year: dl.acm.org\n",
            "Venue: C Zhai, J Lafferty\n",
            "Abstract: … to information retrieval are attractive and promising because they connect the problem of \n",
            "retrieval … smoothing and its influence on retrieval performance. We examine the sensitivity of …\n",
            "\n",
            "Article 5:\n",
            "Title: A latent semantic model with convolutional-pooling structure for information retrieval\n",
            "Authors: Y Shen, X He, J Gao, L Deng, G Mesnil - … on conference on information …, 2014 - dl.acm.org\n",
            "Year: dl.acm.org\n",
            "Venue: Y Shen, X He, J Gao, L Deng, G Mesnil\n",
            "Abstract: … lexical matching for Web document retrieval. This is partially … have also been proposed for \n",
            "information retrieval (IR) [16][32]… as weakly-supervised information in training the model. In …\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "keyword = \"information retrieval\"\n",
        "url = f\"https://scholar.google.com/scholar?q={keyword}&as_ylo=2013&as_yhi=2023\"\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    results = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "    for i, result in enumerate(results[:5]):\n",
        "        title = result.find(\"h3\").text\n",
        "        authors = result.find(\"div\", class_=\"gs_a\").text\n",
        "        year = authors.split(\"-\")[-1].strip()\n",
        "        venue = authors.split(\"-\")[0].strip()\n",
        "        abstract = result.find(\"div\", class_=\"gs_rs\").text if result.find(\"div\", class_=\"gs_rs\") else \"N/A\"\n",
        "        print(f\"Article {i + 1}:\")\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Authors: {authors}\")\n",
        "        print(f\"Year: {year}\")\n",
        "        print(f\"Venue: {venue}\")\n",
        "        print(f\"Abstract: {abstract}\\n\")\n",
        "else:\n",
        "    print(\"Failed to retrieve search results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "## Question 4 (5 points):\n",
        "\n",
        "Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymVNKVi9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "ad24d8d1-1181-493e-e050-29341ca09793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: instaloader in /usr/local/lib/python3.10/dist-packages (4.10)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.10/dist-packages (from instaloader) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JSON Query to graphql/query: HTTP error code 401. [retrying; skip with ^C]\n",
            "JSON Query to graphql/query: HTTP error code 401. [retrying; skip with ^C]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ConnectionException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, host, session, _attempt, response_headers)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HTTP error code {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mis_html_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_graphql_query\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"__a\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"www.instagram.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionException\u001b[0m: HTTP error code 401.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, host, session, _attempt, response_headers)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HTTP error code {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mis_html_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_graphql_query\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"__a\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"www.instagram.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionException\u001b[0m: HTTP error code 401.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, host, session, _attempt, response_headers)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HTTP error code {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mis_html_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_graphql_query\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"__a\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"www.instagram.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionException\u001b[0m: HTTP error code 401.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f620cd1c375e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstaloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_username\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_username\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mposts_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_posts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     post_data = {\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"User_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner_username\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/nodeiterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'page_info'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'has_next_page'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mquery_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'page_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_cursor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edges'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mquery_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edges'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edges'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mpage_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/nodeiterator.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, after)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             data = self._edge_extractor(\n\u001b[0;32m--> 103\u001b[0;31m                 self._context.graphql_query(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpagination_variables\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_referer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mgraphql_query\u001b[0;34m(self, query_hash, variables, referer, rhx_gis)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mtmpsession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x-instagram-gis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_instagram_gis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             resp_json = self.get_json('graphql/query',\n\u001b[0m\u001b[1;32m    484\u001b[0m                                       params={'query_hash': query_hash,\n\u001b[1;32m    485\u001b[0m                                               'variables': variables_json},\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, host, session, _attempt, response_headers)\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_other_query\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rate_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_429\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                 return self.get_json(path=path, params=params, host=host, session=sess, _attempt=_attempt + 1,\n\u001b[0m\u001b[1;32m    449\u001b[0m                                      response_headers=response_headers)\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, host, session, _attempt, response_headers)\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_other_query\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rate_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_429\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                 return self.get_json(path=path, params=params, host=host, session=sess, _attempt=_attempt + 1,\n\u001b[0m\u001b[1;32m    449\u001b[0m                                      response_headers=response_headers)\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/instaloader/instaloadercontext.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, host, session, _attempt, response_headers)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mQueryReturnedNotFoundException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_string\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" [retrying; skip with ^C]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat_at_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionException\u001b[0m: JSON Query to graphql/query: HTTP error code 401."
          ]
        }
      ],
      "source": [
        "!pip install instaloader\n",
        "import instaloader\n",
        "L = instaloader.Instaloader()\n",
        "target_username = \"samantharuthprabhuoffl\"\n",
        "profile = instaloader.Profile.from_username(L.context, target_username)\n",
        "posts_data = []\n",
        "for post in profile.get_posts():\n",
        "    post_data = {\n",
        "        \"User_name\": post.owner_username,\n",
        "        \"Posted_time\": post.date_utc,\n",
        "        \"Text\": post.caption,\n",
        "    }\n",
        "    posts_data.append(post_data)\n",
        "    if len(posts_data) >= 1000:\n",
        "        break\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(posts_data)\n",
        "df.to_csv(\"instagram_posts.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AWoKpYQT9-pa",
        "MlxTLRNm9-pa",
        "px6wgvog9-pa",
        "GT3CNj_V9-pb"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}